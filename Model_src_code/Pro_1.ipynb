{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pro_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCe5b2SRWpCc",
        "outputId": "81761571-886a-4917-ac90-3748d04860e4"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alignments.txt\t\t\t   parallel\t   train_data.json\n",
            "encoder_decoder_with_attention.pt  parallel.tgz    validation_data.json\n",
            "en_hi.pkl\t\t\t   sample_data\n",
            "IITB_input.pkl\t\t\t   test_data.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6aFIJTxRfwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a64df2-2378-4018-8b30-d8df8a53931c"
      },
      "source": [
        " !wget http://www.cfilt.iitb.ac.in/iitb_parallel/iitb_corpus_download/parallel.tgz "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-19 21:45:13--  http://www.cfilt.iitb.ac.in/iitb_parallel/iitb_corpus_download/parallel.tgz\n",
            "Resolving www.cfilt.iitb.ac.in (www.cfilt.iitb.ac.in)... 103.21.127.130\n",
            "Connecting to www.cfilt.iitb.ac.in (www.cfilt.iitb.ac.in)|103.21.127.130|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100235015 (96M) [application/x-gzip]\n",
            "Saving to: ‘parallel.tgz.1’\n",
            "\n",
            "parallel.tgz.1      100%[===================>]  95.59M  9.24MB/s    in 22s     \n",
            "\n",
            "2020-11-19 21:45:36 (4.36 MB/s) - ‘parallel.tgz.1’ saved [100235015/100235015]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uliyazHfRhir",
        "outputId": "d184f1c7-cb14-41af-c45a-86e6aa681c14"
      },
      "source": [
        " !tar -xzvf parallel.tgz\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parallel/\n",
            "parallel/IITB.en-hi.hi\n",
            "parallel/IITB.en-hi.en\n",
            "alignments.txt\t\t\t   parallel\t   test_data.json\n",
            "encoder_decoder_with_attention.pt  parallel.tgz    train_data.json\n",
            "en_hi.pkl\t\t\t   parallel.tgz.1  validation_data.json\n",
            "IITB_input.pkl\t\t\t   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSiWRH7pRiap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71270e85-7cf5-4018-c08d-373da34ad924"
      },
      "source": [
        "import pickle\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "cnt=0;\n",
        "# with open('en_hi.pkl', 'rb') as f:\n",
        "#     data = pickle.load(f)\n",
        "#     # print(pickle.load(f))\n",
        "\n",
        "# print(type(data))\n",
        "# cnt=0\n",
        "# for ind in data.index: \n",
        "#     print(data['english'][ind], data['hindi'][ind])\n",
        "#     cnt+=1\n",
        "#     if cnt>=10:\n",
        "# \t    break \n",
        "cnt=0\n",
        "\n",
        "fe=open(\"parallel/IITB.en-hi.en\",\"rb\")\n",
        "fh=open(\"parallel/IITB.en-hi.hi\",\"rb\")\n",
        "\n",
        "# print(f.readlines())\n",
        "# for x in f.readlines():\n",
        "# \tprint(x)\n",
        "# \tcnt+=1\n",
        "# \tif cnt>=10:\n",
        "# \t\tbreak\n",
        "\n",
        "d=defaultdict(list)\n",
        "for x in fe.readlines():\n",
        "\td['english'].append(x.decode('utf-8'))\n",
        "\t# d['hindi'].append(data['hindi'][ind])\n",
        "\tcnt+=1\n",
        "\tif cnt>=100000:\n",
        "\t\t# print(x)\n",
        "\t\t# print(x.decode('utf-8'))\n",
        "\t\tbreak\n",
        "\t\t\t# if cntt<=10:\n",
        "\t\t# print(x)\n",
        "\t\t# print() \n",
        "\t\t# break\n",
        "cntt=0\n",
        "# with open(\"IITB.en-hi.hi\", 'rb') as f:\n",
        "  # contents = f.readlines()\n",
        "for x in fh.readlines():\n",
        "\t# d['english'].append(x)\n",
        "\td['hindi'].append(x.decode('utf-8'))\n",
        "\tcntt+=1\n",
        "\tif cntt>=100000:\n",
        "\t\t# print(x)\n",
        "\t\t# print() \n",
        "\t\tbreak\n",
        "print(len(d['hindi']))\n",
        "print(len(d['english']))\n",
        "print(cntt,cnt)\n",
        "data = pd.DataFrame(d)\n",
        "print(data.shape)\n",
        "cnt=0\n",
        "# for ind in data.index: \n",
        "    # print( data['english'][ind],data['hindi'][ind])\n",
        "    # cnt+=1\n",
        "    # if cnt>=100000:\n",
        "\t    # break \n",
        "cnt=0\n",
        "\n",
        "# print(d)\n",
        "filename=\"IITB_input.pkl\"\n",
        "outfile=open(filename,'wb')\n",
        "pickle.dump(data,outfile)\n",
        "outfile.close()\n",
        "with open('IITB_input.pkl', 'rb') as f:\n",
        "    dat = pickle.load(f)\n",
        "#     # print(pickle.load(f))\n",
        "\n",
        "print(type(dat))\n",
        "cnt=0\n",
        "for ind in dat.index: \n",
        "    print(dat['english'][ind], dat['hindi'][ind])\n",
        "    cnt+=1\n",
        "    if cnt>=10:\n",
        "\t    break \n",
        "# cnt=0\n",
        "\n",
        "\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000\n",
            "100000\n",
            "100000 100000\n",
            "(100000, 2)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Give your application an accessibility workout\n",
            " अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें\n",
            "\n",
            "Accerciser Accessibility Explorer\n",
            " एक्सेर्साइसर पहुंचनीयता अन्वेषक\n",
            "\n",
            "The default plugin layout for the bottom panel\n",
            " निचले पटल के लिए डिफोल्ट प्लग-इन खाका\n",
            "\n",
            "The default plugin layout for the top panel\n",
            " ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका\n",
            "\n",
            "A list of plugins that are disabled by default\n",
            " उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से निष्क्रिय किया गया है\n",
            "\n",
            "Highlight duration\n",
            " अवधि को हाइलाइट रकें\n",
            "\n",
            "The duration of the highlight box when selecting accessible nodes\n",
            " पहुंचनीय आसंधि (नोड) को चुनते समय हाइलाइट बक्से की अवधि\n",
            "\n",
            "Highlight border color\n",
            " सीमांत (बोर्डर) के रंग को हाइलाइट करें\n",
            "\n",
            "The color and opacity of the highlight border.\n",
            " हाइलाइट किए गए सीमांत का रंग और अपारदर्शिता। \n",
            "\n",
            "Highlight fill color\n",
            " भराई के रंग को हाइलाइट करें\n",
            "\n",
            "alignments.txt\t\t\t   parallel\t   test_data.json\n",
            "encoder_decoder_with_attention.pt  parallel.tgz    train_data.json\n",
            "en_hi.pkl\t\t\t   parallel.tgz.1  validation_data.json\n",
            "IITB_input.pkl\t\t\t   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlAMBN3ddQBK"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw7tSYM9zODV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm \n",
        "\n",
        "# # device = torch.dev\n",
        "#             actual = batch.hindi\n",
        "#             predictions = model(input, actual)\n",
        "# …attention = Attention(512)\n",
        "# dec = Decoder(len(HINDI.vocab), 400, 512, 0.5, attention)\n",
        "# model = Model(enc, dec, device).to(device)\n",
        "\n",
        "# for name, param in model.named_parameters():\n",
        "#         if \"weight\" in name:\n",
        "#             nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "#         else:ice(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evHaIgobzPN7"
      },
      "source": [
        "from torchtext import data\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRUw8UtHzQGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de0cf3c8-e9fa-422b-c3ca-2a15c16d755b"
      },
      "source": [
        "!pip install indic-nlp-library"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.6/dist-packages (0.71)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->indic-nlp-library) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nbI4Sv4zQ6l"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_pickle(\"./IITB_input.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzDhO7XRzYAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0011412d-91bc-4d3b-85bf-afece70d04e9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_temp, test_data_sent = train_test_split(df, test_size = 0.2)\n",
        "train_data_sent, validation_data_sent = train_test_split(train_temp, test_size=0.125)\n",
        "print(test_data_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                 english                                              hindi\n",
            "45917                                    _ Reset fonts\\n                      फोंट फिर से सेट करें (_ R) \\n\n",
            "22155                  Diff local tree with repository\\n                          डिफ स्थानीय ट्री के साथ\\n\n",
            "38714                Save project as a plain text list\\n               बतौर सादा पाठ सूची परियोजना सहेजें\\n\n",
            "7748      Berkeley Software Distribution License (BSD)\\n    Berkeley Software Distribution License (BSD) \\n\n",
            "45455                        Disable GL Vertex Buffers\\n                    GL शीर्ष उद्धरण को अक्षम करें\\n\n",
            "...                                                  ...                                                ...\n",
            "8675                                       Description\\n                                            वर्णन\\n\n",
            "43450                              Load asynchronously\\n                                  अतुल्यकालित लोड\\n\n",
            "62449  Could not verify this certificate because it h...  इस प्रमाणपत्र को सत्यापित नहीं कर सके चूंकि इस...\n",
            "56356                                      _ Add Group\\n  समूह जोड़ें (_ A) verb in a column header disp...\n",
            "48894  I do not want to sign up for the Ekiga Call Ou...  मैं एकिगा कॉल आउट सेवा के लिए साइन अप नहीं करन...\n",
            "\n",
            "[20000 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jwz1gIJzZW0"
      },
      "source": [
        "train_data_sent.to_json('train_data.json', orient='records', lines=True)\n",
        "validation_data_sent.to_json('validation_data.json', orient='records', lines=True)\n",
        "test_data_sent.to_json('test_data.json', orient='records', lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg8eDkdwzaCD"
      },
      "source": [
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "# from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
        "# factory=IndicNormalizerFactory()\n",
        "# normalizer=factory.get_normalizer(\"hi\", remove_nuktas=False)\n",
        "\n",
        "def hindi_tokenizer(sentence):\n",
        "    # sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n",
        "    # return [word for word in indic_tokenize.trivial_tokenize(normalizer.normalize(sentence.strip().split(\"-\")[-1].strip()))]\n",
        "    return [word for word in indic_tokenize.trivial_tokenize(sentence.strip().split(\"-\")[-1].strip())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-h01f0gzbu6"
      },
      "source": [
        "import spacy\n",
        "en_tokenizer = spacy.load('en')\n",
        "\n",
        "def english_tokenizer(sentence):\n",
        "    # sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n",
        "    # sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n",
        "    return [word.text for word in en_tokenizer.tokenizer(sentence.strip().split(\"-\")[-1].strip())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXqy-vRbzchP"
      },
      "source": [
        "ENGLISH = Field(tokenize = english_tokenizer, init_token = '<sos>', eos_token = '<eos>', lower = True)\n",
        "HINDI = Field(tokenize = hindi_tokenizer, init_token = '<sos>', eos_token = '<eos>')\n",
        "fields = {'english': ('english', ENGLISH), 'hindi': ('hindi', HINDI)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugbEnvKizdbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add43963-1c96-4af9-b430-b37318e3d9ac"
      },
      "source": [
        "train_data, validation_data, test_data = data.TabularDataset.splits(path = '',\n",
        "                                        train = 'train_data.json',\n",
        "                                        validation = 'validation_data.json',\n",
        "                                        test = 'test_data.json',\n",
        "                                        format = 'json',\n",
        "                                        fields = fields)\n",
        "\n",
        "print(len(train_data), len(validation_data), len(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70000 10000 20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SeupvQbzeJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35bf034c-caf9-4abc-9d5a-330d91a1b051"
      },
      "source": [
        "# print(train_data_sent)\n",
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'english': ['view', 'or', 'change', 'your', 'exchange', 'calendar', 'delegation', 'settings'], 'hindi': ['दृश्य', 'या', 'बदलें', 'अपने', 'विनिमय', 'केलेण्डर', 'डेलिगेशन', 'जमावट']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hRJ3EFTzfAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c82711-79d6-4f03-a00b-a895b257d71c"
      },
      "source": [
        "ENGLISH.build_vocab(train_data, min_freq=2)\n",
        "HINDI.build_vocab(train_data, min_freq=2)\n",
        "\n",
        "print(f\"English: {len(ENGLISH.vocab)}, Hindi: {len(HINDI.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: 5410, Hindi: 7086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIASQiDlcNcQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aZtDNfozfwe"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, p):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.word_embeddings = nn.Embedding(input_size, embedding_size) \n",
        "        self.gru = nn.GRU(embedding_size, hidden_size, bidirectional=True)\n",
        "        self.linear = nn.Linear(2 * hidden_size, hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "    def forward(self, input):\n",
        "        embeddings = self.dropout(self.word_embeddings(input))\n",
        "        o, h = self.gru(embeddings) \n",
        "        h = torch.tanh(self.linear(torch.cat((h[-2,:,:], h[-1,:,:]), dim = 1)))\n",
        "\n",
        "        return o, h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXSDnTWL0Fay"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attention_layer = nn.Linear(3 * hidden_size, hidden_size)\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        "        \n",
        "    def forward(self, h, eo):\n",
        "        h = h.unsqueeze(1).repeat(1, eo.shape[0], 1)\n",
        "        eo = eo.permute(1, 0, 2)\n",
        "        e = torch.tanh(self.attention_layer(torch.cat((h, eo), dim = 2))) \n",
        "        alpha = self.linear(e).squeeze(2)\n",
        "        \n",
        "        return F.softmax(alpha, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scR1Emc70G9P"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embedding_size, hidden_size, p, attention):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.word_embeddings = nn.Embedding(output_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size + 2 * hidden_size, hidden_size)\n",
        "        self.linear = nn.Linear(embedding_size + 3 * hidden_size, output_size)\n",
        "        self.output_size = output_size\n",
        "        self.attention = attention\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, input, h, eo):        \n",
        "        embeddings = self.dropout(self.word_embeddings(input.unsqueeze(0)))\n",
        "        alpha = self.attention(h, eo).unsqueeze(1)\n",
        "        eo = eo.permute(1, 0, 2)\n",
        "        w = torch.bmm(alpha, eo).permute(1, 0, 2)\n",
        "        o, h = self.gru(torch.cat((embeddings, w), dim = 2), h.unsqueeze(0))\n",
        "        predictions = self.linear(torch.cat((o, w, embeddings), dim = 2).squeeze(0))\n",
        "\n",
        "        return predictions, h.squeeze(0),alpha.squeeze(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmuo_R0I0LXz"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "         \n",
        "    def forward(self, input, actual):\n",
        "        \n",
        "        eo, h = self.encoder(input)\n",
        "        input = actual[0, :]\n",
        "        predictions = torch.zeros(actual.shape[0], actual.shape[1], self.decoder.output_size).to(self.device)\n",
        "\n",
        "        for t in range(1, actual.shape[0]):\n",
        "            o, h,_ = self.decoder(input, h, eo)\n",
        "            predictions[t] = o\n",
        "            predicted = o.argmax(1) \n",
        "            input = predicted\n",
        "\n",
        "        return predictions      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_ENDj8kuYFE"
      },
      "source": [
        "def train(model, train_data_iterator, optimizer, criterion):\n",
        "    \n",
        "    model.train()    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in train_data_iterator:\n",
        "        input = batch.english\n",
        "        actual = batch.hindi\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(input, actual)\n",
        "        vocab_size = predictions.shape[-1]\n",
        "        predictions = predictions[1:].view(-1, vocab_size)\n",
        "        actual = actual[1:].view(-1) # flattening the sentence_length x batch_size dimensions because cross entropy loss needs 1d shape and also removing the sos token\n",
        "        loss = criterion(predictions, actual)\n",
        "        loss.backward()   \n",
        "        total_loss += loss.item()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        \n",
        "    average_loss = total_loss / len(train_data_iterator)\n",
        "    return average_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1N5ryYcu6OD"
      },
      "source": [
        "def evaluate(model, data_iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in data_iterator:\n",
        "            input = batch.english\n",
        "            actual = batch.hindi\n",
        "            predictions = model(input, actual)\n",
        "            vocab_size = predictions.shape[-1]\n",
        "            predictions = predictions[1:].view(-1, vocab_size)\n",
        "            actual = actual[1:].view(-1) # flattening the sentence_length x batch_size dimensions because cross entropy loss needs 1d shape and also removing the sos token\n",
        "            loss = criterion(predictions, actual)\n",
        "            total_loss += loss.item()\n",
        "    \n",
        "    average_loss = total_loss / len(data_iterator)\n",
        "    return average_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEY14fEp0Npb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab3b60a-c510-488d-ba60-73b7e6fb4669"
      },
      "source": [
        "enc = Encoder(len(ENGLISH.vocab), 400, 512, 0.5)\n",
        "attention = Attention(512)\n",
        "dec = Decoder(len(HINDI.vocab), 400, 512, 0.5, attention)\n",
        "model = Model(enc, dec, device).to(device)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "        if \"weight\" in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (word_embeddings): Embedding(5410, 400)\n",
              "    (gru): GRU(400, 512, bidirectional=True)\n",
              "    (linear): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (word_embeddings): Embedding(7086, 400)\n",
              "    (gru): GRU(1424, 512)\n",
              "    (linear): Linear(in_features=1936, out_features=7086, bias=True)\n",
              "    (attention): Attention(\n",
              "      (attention_layer): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (linear): Linear(in_features=512, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBciEbuz01bx"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = HINDI.vocab.stoi[HINDI.pad_token])\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, validation_data, test_data), batch_size = 64, device = device, sort = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOZbMs9C0_J1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af1305f-b965-4e1d-80e8-1d3663e7c0e1"
      },
      "source": [
        "best_loss = 1e9\n",
        "\n",
        "train_data_iterator, validation_data_iterator, test_data_iterator = BucketIterator.splits((train_data, validation_data, test_data), batch_size = 32, device = device, sort = False)\n",
        "\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "for epoch in tqdm(range(15)):\n",
        "    train_loss = train(model, train_data_iterator, optimizer, criterion)\n",
        "    valid_loss = evaluate(model, validation_data_iterator, criterion)\n",
        "    training_losses.append(np.exp(train_loss))\n",
        "    validation_losses.append(np.exp(valid_loss))\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'encoder_decoder_with_attention.pt')\n",
        "    \n",
        "    print(f\"Epoch: {epoch + 1}\")\n",
        "    print(f\"Train Loss (exponent to analyse better): {np.exp(train_loss):.3f}\")\n",
        "    print(f\"Val. Loss (exponent to analyse better): {np.exp(valid_loss):.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  7%|▋         | 1/15 [06:16<1:27:57, 376.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss (exponent to analyse better): 47.526\n",
            "Val. Loss (exponent to analyse better): 10.216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDXM2g8FwN0J"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(training_losses, label = 'Training')\n",
        "plt.plot(validation_losses, label = 'Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoYOxhAO1A06"
      },
      "source": [
        "model.load_state_dict(torch.load('encoder_decoder_with_attention.pt'))\n",
        "test_loss = evaluate(model, test_data_iterator, criterion)\n",
        "print(f\"Test Loss (exponent to analyse better): {np.exp(test_loss):.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-nRsf3x2dpb"
      },
      "source": [
        "def translate(text,max_len=50):\n",
        "  model.eval()\n",
        "  tokens = english_tokenizer(text)\n",
        "  tokens = [ENGLISH.init_token] + tokens + [ENGLISH.eos_token]\n",
        "  src_indexes = [ENGLISH.vocab.stoi[token] for token in tokens]\n",
        "  src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "  src_tensor = src_tensor.reshape(-1,1)\n",
        "  with torch.no_grad():\n",
        "      encoder_outputs, hidden = model.encoder(src_tensor)\n",
        "  trg_indexes = [HINDI.vocab.stoi[HINDI.init_token]]\n",
        "\n",
        "  attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "  for i in range(max_len):\n",
        "\n",
        "      trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "              \n",
        "      with torch.no_grad():\n",
        "          output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
        "\n",
        "      attentions[i] = attention\n",
        "          \n",
        "      pred_token = output.argmax(1).item()\n",
        "      \n",
        "      trg_indexes.append(pred_token)\n",
        "\n",
        "      if pred_token == HINDI.vocab.stoi[HINDI.eos_token]:\n",
        "          break\n",
        "  \n",
        "  trg_tokens = [HINDI.vocab.itos[i] for i in trg_indexes]\n",
        "  \n",
        "  return trg_tokens[1:], attentions[:len(trg_tokens)-1]\n",
        "\n",
        "  # output = model(src_tensor, src_tensor)\n",
        "  # output_dim = output.shape[-1]\n",
        "  # output = output.view(-1, output_dim)\n",
        "  # indices = torch.argmax(output,dim=1).tolist()\n",
        "  # return [HINDI.vocab.itos[x] for x in indices]\n",
        "\n",
        "print(translate(\"man speaking native language:\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HJaGR_L5SyQ"
      },
      "source": [
        "# for i, (eng_sentence, hin_sentence) in enumerate(zip(test_data_sent[\"english\"], test_data_sent[\"hindi\"])):\n",
        "#   print(eng_sentence, hin_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vkCUGoO0Zw2"
      },
      "source": [
        "# import csv\n",
        "\n",
        "file1 = open(\"alignments.txt\",\"a\")#write mode \n",
        "\n",
        "# translated_sentences = open('Paper2.csv','w')\n",
        "# csvwriter = csv.writer(translated_sentences)\n",
        "# csvwriter.writerow(['Input','Predicted','Actual'])  \n",
        "co=0\n",
        "for i, (eng_sentence, hin_sentence) in enumerate(zip(test_data_sent[\"english\"], test_data_sent[\"hindi\"])):\n",
        "\n",
        "  hindi_predicted ,attention = translate(eng_sentence)\n",
        "  print(hindi_predicted)\n",
        "  hindlen=hindi_predicted\n",
        "  tokens = english_tokenizer(eng_sentence)\n",
        "  tok=english_tokenizer(eng_sentence)\n",
        "  print(tokens)\n",
        "\n",
        "  # print(hindi_predicted.strip())\n",
        "  # print(eng_sentence)\n",
        "  hindi_predicted = \" \".join(list(filter(lambda x: x != '<eos>', hindi_predicted))).strip()\n",
        "  tokens=\" \".join(list(filter(lambda x: x != '<eos>', tokens))).strip()\n",
        "  # print(hindi_predicted)\n",
        "  # print(hindi_predicted.strip())\n",
        "  # print(eng_sentence.strip())\n",
        "  # print(type(eng_sentence.strip()))\n",
        "  # print(type(eng_sentence))\n",
        "  # print(type([ENGLISH.init_token]))\n",
        "  # print([ENGLISH.init_token])\n",
        "  input_sentence=\"0 ||| \"\n",
        "  input_sentence=input_sentence + (hindi_predicted).strip() + \" ||| 0 ||| \"\n",
        "  input_sentence+=\" \".join([ENGLISH.init_token])\n",
        "  input_sentence=input_sentence + \" \" + (tokens).strip() + \" ||| \" + str(len(tok)+2) + \" \" + str(len(hindlen))\n",
        "  # print(input_sentence)\n",
        "  file1.write(input_sentence)\n",
        "  file1.write(\"\\n\")\n",
        "\n",
        "  attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "  # print(attention)\n",
        "  # print(type(attention))\n",
        "  input_mat=\"\"\n",
        "\n",
        "  for x in attention:\n",
        "    input_mat=\"\"\n",
        "    for y in x:\n",
        "      input_mat+=str(y)+ \" \"\n",
        "    print(input_mat)\n",
        "    file1.write(input_mat)\n",
        "    file1.write(\"\\n\")    \n",
        "  # print(file1)\n",
        "  file1.write(\"\\n\")\n",
        "  # file1.close()\n",
        "  co+=1\n",
        "  if(co>=1):\n",
        "    break\n",
        "\n",
        "\n",
        "file1.close()\n",
        "\n",
        "\n",
        "\n",
        "  # csvwriter.writerow([eng_sentence.strip(), hindi_predicted.strip(), hin_sentence.strip()])\n",
        "\n",
        "  # print(eng_sentence.strip())\n",
        "  # print(hindi_predicted)\n",
        "  # print()\n",
        "  # print()\n",
        "\n",
        "# translated_sentences.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETEPL0HP_2dc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}